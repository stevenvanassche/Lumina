# Lumina Docker Environment Configuration
# Copy this file to .env and adjust values for your setup

# ============================================================================
# PATH CONFIGURATION
# ============================================================================

# Photos directory (required) - mount your photos here
# This directory will contain your photo files, and XMP and JSON sidecar files
PHOTOS_PATH=./photos

# Cache directory (required) - for models, checkpoints, logs, and temporary files
# All persistent data (models, logs, checkpoints) will be stored here
# This directory will contain subdirectories:
#   - models/           (AI models and their caches)
#     - RtDetr/         (RT-DETR model weights)
#     - torch/          (PyTorch cache)
#     - huggingface/    (HuggingFace/Transformers cache for RT-DETR)
#     - timm/           (timm cache for iNat21 model)
#   - checkpoints/      (resumable processing state)
#   - logs/             (application logs)
CACHE_PATH=./cache

# ============================================================================
# EXECUTION CONFIGURATION
# ============================================================================

# Number of worker processes per processor (default: 2 for CPU, 2 for GPU)
# Higher values = more parallel processing but more memory usage
# CPU: recommended 1-5, GPU: recommended 1-10
LUMINA_WORKERS=2

# Batch size for processing (default: 5 for CPU, 8 for GPU)
# Higher values = better GPU utilization but more memory
# CPU: recommended 3-5, GPU: recommended 5-10
LUMINA_BATCH_SIZE=5

# Number of parallel batches in pipeline (default: 2 for CPU, 10 for GPU)
# Higher values = more throughput but more memory
LUMINA_PARALLEL_BATCHES=2

# Ray CPU allocation (default: auto-detect)
# Set to specific number to override auto-detection
# Use 'auto' for automatic detection based on available cores
LUMINA_RAY_CPUS=auto

# PyTorch CPU threads for AI processors (default: 4 for CPU, not used for GPU)
# Number of threads PyTorch uses for intra-op parallelism (matrix operations)
# Higher values = better CPU utilization but more memory
# Recommended: 2-8 for CPU mode, leave empty for GPU mode
LUMINA_TORCH_THREADS=4

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LUMINA_LOG_LEVEL=INFO

# Enable/disable checkpoints for resumable processing
# Set to 'true' to enable checkpoint-based resumption
LUMINA_ENABLE_CHECKPOINTS=true

# Execution mode: quick_scan, normal_scan, force_update
# - quick_scan: Only process photos without existing metadata
# - normal_scan: Process photos without metadata or with outdated metadata
# - force_update: Reprocess all photos regardless of existing metadata
LUMINA_EXECUTION_MODE=force_update

# ============================================================================
# DOCKER COMPOSE PROFILES
# ============================================================================
# Use these to select which variant to run:
#
# CPU variant (no GPU):
#   docker-compose --profile cpu up
#
# GPU variant (requires NVIDIA GPU):
#   docker-compose --profile gpu up

# ============================================================================
# QUICK START EXAMPLES
# ============================================================================

# Basic CPU setup (minimal resources):
# LUMINA_WORKERS=1
# LUMINA_BATCH_SIZE=3
# LUMINA_PARALLEL_BATCHES=1
# LUMINA_RAY_CPUS=4
# LUMINA_TORCH_THREADS=1

# Optimized CPU setup (8+ cores):
# LUMINA_WORKERS=5
# LUMINA_BATCH_SIZE=5
# LUMINA_PARALLEL_BATCHES=5
# LUMINA_RAY_CPUS=auto
# LUMINA_TORCH_THREADS=4

# Basic GPU setup:
# LUMINA_WORKERS=2
# LUMINA_BATCH_SIZE=7
# LUMINA_PARALLEL_BATCHES=4
# LUMINA_RAY_CPUS=auto

# High-performance GPU setup (high-end GPU):
# LUMINA_WORKERS=10
# LUMINA_BATCH_SIZE=10
# LUMINA_PARALLEL_BATCHES=20
# LUMINA_RAY_CPUS=auto
