# Lumina v1.1 - GPU Configuration Example
# Copy this file to .env and adjust values for your setup
#
# For CPU-only setup, see .env.cpu.example

# ============================================================================
# PATH CONFIGURATION (Required)
# ============================================================================

# Photos directory - mount your photos here
# Lumina will scan this directory and write XMP sidecar files here
PHOTOS_PATH="/path/to/your/photos"

# Cache directory - for models, checkpoints, logs, and temporary files
# Subdirectories: models/, checkpoints/, logs/, image_cache/
# First run will download ~2GB of AI models to this location
CACHE_PATH="/path/to/cache"

# ============================================================================
# EXECUTION CONFIGURATION
# ============================================================================

# Number of worker processes per processor
# Higher = more parallel processing, more GPU memory
# Recommended: 1 for most GPUs (GPU is the bottleneck, not CPU)
LUMINA_WORKERS=1

# Batch size for processing
# Photos processed together per GPU batch
# Recommended: 10-20 for 8GB+ VRAM, 5-10 for 4-6GB VRAM
LUMINA_BATCH_SIZE=10

# Number of parallel batches in pipeline
# Higher = more throughput, more memory
# Recommended: 2-4 for GPU mode
LUMINA_PARALLEL_BATCHES=3

# Ray CPU allocation
# 'auto' = detect available cores (recommended)
# Or set explicit number (e.g., 4, 8)
LUMINA_RAY_CPUS=auto

# PyTorch CPU threads for non-GPU operations
# Threads used for data loading and preprocessing
# Recommended: 4-8 for GPU mode
LUMINA_TORCH_THREADS=4

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LUMINA_LOG_LEVEL=INFO

# Enable checkpoints for resumable processing
# If interrupted, processing resumes from last checkpoint
LUMINA_ENABLE_CHECKPOINTS=true

# Execution mode:
# - quick_scan: Only process photos without existing metadata (fastest)
# - normal_scan: Process photos with outdated metadata
# - force_update: Reprocess all photos (slowest, but most thorough)
LUMINA_EXECUTION_MODE=normal_scan

# ============================================================================
# WATCH MODE (New in v1.1)
# ============================================================================
# Watch mode enables continuous monitoring for new photos.
# After the initial scan completes, Lumina keeps running and automatically
# processes any new photos added to the photos directory.

# Watch mode: on/off
# - off: Scan existing photos, then exit (default, good for batch processing)
# - on: Scan existing photos, then continue watching for new ones
LUMINA_WATCH_MODE=off

# Model idle timeout: controls GPU/memory usage during watch mode
# - auto: Smart default (300s timeout when watch=on, not relevant when watch=off)
# - disabled: Models stay loaded permanently (fastest response time)
# - <seconds>: Unload after N seconds idle (e.g., 300 = 5 minutes)
#
# GPU MEMORY NOTE: In watch mode with 'auto', models unload after 5 minutes
# of idle to free ~2-4GB VRAM. Set 'disabled' to keep models loaded for
# instant processing (recommended if GPU has sufficient memory).
LUMINA_MODEL_IDLE_TIMEOUT=auto

# Watch mode auto-exit: exit after N seconds of inactivity (no new files)
# - 0 or unset: Disabled, keep watching indefinitely
# - <seconds>: Exit after this many seconds of idle time (e.g., 60 = 1 minute)
# Useful for batch-style processing where you want to exit after all files are done
LUMINA_WATCH_IDLE_EXIT=0

# ============================================================================
# RAW FILE SUPPORT (New in v1.1)
# ============================================================================
# Control how Lumina handles RAW files (NEF, CR2, ARW, etc.)
#
# Available modes:
# - on (default): RAW+JPEG pairing - group RAW+JPEG as single photo
#       JPEG is used for AI analysis (faster)
#       One XMP sidecar is created for the pair (shared by both files)
#       RAW-only files are converted to JPEG for analysis
#       If you only have JPEG files, this works fine too (no overhead)
# - off: Only process JPEG/PNG files (ignores RAW files)
# - only: Only process RAW files (no JPEG/PNG)
#
LUMINA_RAW_SUPPORT=on

# ============================================================================
# RUN COMMAND
# ============================================================================
# Start Lumina with GPU profile:
#   docker compose --profile gpu up
#
# Or with watch mode:
#   LUMINA_WATCH_MODE=on docker compose --profile gpu up

# ============================================================================
# QUICK START PROFILES
# ============================================================================

# Entry-level GPU (4GB VRAM, e.g., GTX 1650):
# LUMINA_WORKERS=1
# LUMINA_BATCH_SIZE=5
# LUMINA_PARALLEL_BATCHES=2
# LUMINA_RAY_CPUS=auto
# LUMINA_TORCH_THREADS=4

# Mid-range GPU (8GB VRAM, e.g., RTX 3070, RTX 4060):
# LUMINA_WORKERS=1
# LUMINA_BATCH_SIZE=10
# LUMINA_PARALLEL_BATCHES=3
# LUMINA_RAY_CPUS=auto
# LUMINA_TORCH_THREADS=4

# High-end GPU (12GB+ VRAM, e.g., RTX 3080, RTX 4080):
# LUMINA_WORKERS=1
# LUMINA_BATCH_SIZE=20
# LUMINA_PARALLEL_BATCHES=4
# LUMINA_RAY_CPUS=auto
# LUMINA_TORCH_THREADS=8

# Multi-GPU setup (advanced):
# For multiple GPUs, run separate containers per GPU
# or configure Ray for multi-GPU distribution
# See documentation for multi-GPU setup

